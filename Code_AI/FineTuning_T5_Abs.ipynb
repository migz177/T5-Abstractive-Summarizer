{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IM_xLRzQfW0d"},"outputs":[],"source":["!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQptRe39fW7N"},"outputs":[],"source":["!pip install datasets transformers accelerate torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYHqqW-pfW-U"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from datasets import load_dataset, Dataset\n","from transformers import BertTokenizer, BertForSequenceClassification, DataCollatorForSeq2Seq, EncoderDecoderModel\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2i0P_BObfXBc"},"outputs":[],"source":["filetrain_path = \"/content/drive/MyDrive/Summarizer_AI/Dataset/Fixed/train_df.csv\"\n","filedev_path = \"/content/drive/MyDrive/Summarizer_AI/Dataset/Fixed/dev_df.csv\"\n","\n","train_df = pd.read_csv(filetrain_path)\n","dev_df = pd.read_csv(filedev_path)\n","\n","# Basic overview\n","print(train_df.shape)\n","print(train_df.info())\n","print(train_df.isnull().sum())\n","\n","print(dev_df.shape)\n","print(dev_df.info())\n","print(dev_df.isnull().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAoTgM4fgeBC"},"outputs":[],"source":["columns_to_delete = ['Unnamed: 0']\n","\n","train_df = train_df.drop(columns=columns_to_delete)\n","dev_df = dev_df.drop(columns=columns_to_delete)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2r9c3A3geEG"},"outputs":[],"source":["train_df.iloc[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wxlKqx4hAFf"},"outputs":[],"source":["dev_df.iloc[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqvWsHxageJ1"},"outputs":[],"source":["import torch\n","print(f\"PyTorch Version: {torch.__version__}\")\n","print(f\"CUDA Available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnS8uNWfgeMb"},"outputs":[],"source":["device = 'cuda'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9BHb4wdgroG"},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","model_name = \"cahya/t5-base-indonesian-summarization-cased\"\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"to3KWe4IgySa"},"outputs":[],"source":["train_dataset = Dataset.from_pandas(train_df)\n","dev_dataset = Dataset.from_pandas(dev_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iV4zQKdWgyVg"},"outputs":[],"source":["def preprocess_function_abstractive(examples):\n","\n","    abstractive_summary = [\n","        \" \".join(summary) if isinstance(summary, list) else summary\n","        for summary in examples[\"abstractive_summary\"]\n","    ]  # List comprehension for handling list summaries\n","\n","    # Tokenize the text input\n","    model_inputs = tokenizer(\n","        examples[\"original_text\"], max_length=512, truncation=True, padding=\"max_length\"\n","    )\n","\n","    # Tokenize the combined summary (labels)\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(\n","            abstractive_summary, max_length=256, truncation=True, padding=\"max_length\"\n","        )\n","\n","    # Add the tokenized labels to the model inputs\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_aC6kkngyYX"},"outputs":[],"source":["tokenized_train_dataset = train_dataset.map(preprocess_function_abstractive, batched=True)\n","tokenized_dev_dataset = dev_dataset.map(preprocess_function_abstractive, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rf5YPQUSgybO"},"outputs":[],"source":["print(train_dataset[0])\n","print(tokenized_train_dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jV4ym-j_hyDL"},"outputs":[],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./t5_summarization\", #untuk menyimpan hasil\n","    num_train_epochs=3,  #berapa kali train\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    learning_rate=2e-5,\n","    gradient_accumulation_steps=4,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=500,\n","    save_total_limit=3,\n","    save_strategy=\"steps\",\n","    save_steps=500,\n","    logging_dir='./logs',\n","    logging_steps=100,\n","    report_to=\"none\",\n","    predict_with_generate=True,  # Tambahkan untuk evaluasi generasi teks\n","    generation_max_length=128,   # Panjang maksimal teks yang dihasilkan\n","    generation_num_beams=4       # Beam search untuk generasi teks\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LPst05DBhyIF"},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    model=model,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Mu_d6y2ThyLv"},"outputs":[],"source":["trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_dev_dataset,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ftRV1O_hieBj"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1LZQoMfj_LC"},"outputs":[],"source":["import os\n","\n","# Define the path in Google Drive where you want to save the model\n","output_dir = '/content/drive/MyDrive/Summarizer_AI/Dataset/Hasil_FineTuning/T5:Abstractive'\n","\n","# Create the directory if it doesn't exist\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Save the trained model and tokenizer\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","print(f\"Model saved to {output_dir}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1KWJy3AOCFE69_QwMdbImIvWxQ3bbl4YK","authorship_tag":"ABX9TyMXxEuKlToXwaOgG4mKV0EJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}